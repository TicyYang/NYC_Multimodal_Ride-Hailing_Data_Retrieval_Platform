{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bce9b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----+----+-----+---+----+------------+-------+----------+-----+-----------+------------+-------------------+------------------+\n",
      "|__index_level_0__|Name|year|month|day|hour|PULocationID|weekday|is_holiday|count|        lat|         lon|           TempTime|            countN|\n",
      "+-----------------+----+----+-----+---+----+------------+-------+----------+-----+-----------+------------+-------------------+------------------+\n",
      "|         20013480|lyft|2022|    1|  1|   0|           3|      5|      true|   16|40.86429404|-73.84650986|2022-01-01 00:00:00|16.666666666666668|\n",
      "|         20013481|lyft|2022|    1|  1|   1|           3|      5|      true|   21|40.86429404|-73.84650986|2022-01-01 01:00:00|              14.0|\n",
      "|         20013482|lyft|2022|    1|  1|   2|           3|      5|      true|    6|40.86429404|-73.84650986|2022-01-01 02:00:00| 6.666666666666667|\n",
      "|         20013483|lyft|2022|    1|  1|   3|           3|      5|      true|   11|40.86429404|-73.84650986|2022-01-01 03:00:00| 6.333333333333333|\n",
      "|         20013484|lyft|2022|    1|  1|   4|           3|      5|      true|   10|40.86429404|-73.84650986|2022-01-01 04:00:00| 6.333333333333333|\n",
      "|         20013485|lyft|2022|    1|  1|   5|           3|      5|      true|    7|40.86429404|-73.84650986|2022-01-01 05:00:00| 5.333333333333333|\n",
      "|         20013486|lyft|2022|    1|  1|   6|           3|      5|      true|    9|40.86429404|-73.84650986|2022-01-01 06:00:00|               6.0|\n",
      "|         20013487|lyft|2022|    1|  1|   7|           3|      5|      true|    6|40.86429404|-73.84650986|2022-01-01 07:00:00| 4.666666666666667|\n",
      "|         20013488|lyft|2022|    1|  1|   8|           3|      5|      true|    5|40.86429404|-73.84650986|2022-01-01 08:00:00| 7.666666666666667|\n",
      "|         20013489|lyft|2022|    1|  1|   9|           3|      5|      true|    7|40.86429404|-73.84650986|2022-01-01 09:00:00|               9.0|\n",
      "|         20013490|lyft|2022|    1|  1|  10|           3|      5|      true|    2|40.86429404|-73.84650986|2022-01-01 10:00:00| 6.333333333333333|\n",
      "|         20013491|lyft|2022|    1|  1|  11|           3|      5|      true|    5|40.86429404|-73.84650986|2022-01-01 11:00:00| 5.666666666666667|\n",
      "|         20013492|lyft|2022|    1|  1|  12|           3|      5|      true|   11|40.86429404|-73.84650986|2022-01-01 12:00:00|10.333333333333334|\n",
      "|         20013493|lyft|2022|    1|  1|  13|           3|      5|      true|    8|40.86429404|-73.84650986|2022-01-01 13:00:00|10.333333333333334|\n",
      "|         20013494|lyft|2022|    1|  1|  14|           3|      5|      true|   10|40.86429404|-73.84650986|2022-01-01 14:00:00|11.666666666666666|\n",
      "|         20013495|lyft|2022|    1|  1|  15|           3|      5|      true|   11|40.86429404|-73.84650986|2022-01-01 15:00:00|14.666666666666666|\n",
      "|         20013496|lyft|2022|    1|  1|  16|           3|      5|      true|   13|40.86429404|-73.84650986|2022-01-01 16:00:00|12.666666666666666|\n",
      "|         20013497|lyft|2022|    1|  1|  17|           3|      5|      true|   11|40.86429404|-73.84650986|2022-01-01 17:00:00|              15.0|\n",
      "|         20013498|lyft|2022|    1|  1|  18|           3|      5|      true|    9|40.86429404|-73.84650986|2022-01-01 18:00:00|14.666666666666666|\n",
      "|         20013499|lyft|2022|    1|  1|  19|           3|      5|      true|   11|40.86429404|-73.84650986|2022-01-01 19:00:00|              10.0|\n",
      "+-----------------+----+----+-----+---+----+------------+-------+----------+-----+-----------+------------+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler, MinMaxScaler\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# 创建一个Spark会话\n",
    "spark = SparkSession.builder.appName(\"MySparkSession\").getOrCreate()\n",
    "\n",
    "# 加载Parquet文件\n",
    "df = spark.read.parquet('../time_series/TS6/TS6-2022.parquet')\n",
    "df = df.select(\"__index_level_0__\", *df.columns[:-1])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b163d42",
   "metadata": {},
   "source": [
    "#### 將布林值轉成0跟1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "944b8194",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----+----+-----+---+----+------------+-------+----------+-----+-----------+------------+-------------------+------------------+\n",
      "|__index_level_0__|Name|year|month|day|hour|PULocationID|weekday|is_holiday|count|        lat|         lon|           TempTime|            countN|\n",
      "+-----------------+----+----+-----+---+----+------------+-------+----------+-----+-----------+------------+-------------------+------------------+\n",
      "|         20013480|lyft|2022|    1|  1|   0|           3|      5|         1|   16|40.86429404|-73.84650986|2022-01-01 00:00:00|16.666666666666668|\n",
      "|         20013481|lyft|2022|    1|  1|   1|           3|      5|         1|   21|40.86429404|-73.84650986|2022-01-01 01:00:00|              14.0|\n",
      "|         20013482|lyft|2022|    1|  1|   2|           3|      5|         1|    6|40.86429404|-73.84650986|2022-01-01 02:00:00| 6.666666666666667|\n",
      "|         20013483|lyft|2022|    1|  1|   3|           3|      5|         1|   11|40.86429404|-73.84650986|2022-01-01 03:00:00| 6.333333333333333|\n",
      "|         20013484|lyft|2022|    1|  1|   4|           3|      5|         1|   10|40.86429404|-73.84650986|2022-01-01 04:00:00| 6.333333333333333|\n",
      "|         20013485|lyft|2022|    1|  1|   5|           3|      5|         1|    7|40.86429404|-73.84650986|2022-01-01 05:00:00| 5.333333333333333|\n",
      "|         20013486|lyft|2022|    1|  1|   6|           3|      5|         1|    9|40.86429404|-73.84650986|2022-01-01 06:00:00|               6.0|\n",
      "|         20013487|lyft|2022|    1|  1|   7|           3|      5|         1|    6|40.86429404|-73.84650986|2022-01-01 07:00:00| 4.666666666666667|\n",
      "|         20013488|lyft|2022|    1|  1|   8|           3|      5|         1|    5|40.86429404|-73.84650986|2022-01-01 08:00:00| 7.666666666666667|\n",
      "|         20013489|lyft|2022|    1|  1|   9|           3|      5|         1|    7|40.86429404|-73.84650986|2022-01-01 09:00:00|               9.0|\n",
      "|         20013490|lyft|2022|    1|  1|  10|           3|      5|         1|    2|40.86429404|-73.84650986|2022-01-01 10:00:00| 6.333333333333333|\n",
      "|         20013491|lyft|2022|    1|  1|  11|           3|      5|         1|    5|40.86429404|-73.84650986|2022-01-01 11:00:00| 5.666666666666667|\n",
      "|         20013492|lyft|2022|    1|  1|  12|           3|      5|         1|   11|40.86429404|-73.84650986|2022-01-01 12:00:00|10.333333333333334|\n",
      "|         20013493|lyft|2022|    1|  1|  13|           3|      5|         1|    8|40.86429404|-73.84650986|2022-01-01 13:00:00|10.333333333333334|\n",
      "|         20013494|lyft|2022|    1|  1|  14|           3|      5|         1|   10|40.86429404|-73.84650986|2022-01-01 14:00:00|11.666666666666666|\n",
      "|         20013495|lyft|2022|    1|  1|  15|           3|      5|         1|   11|40.86429404|-73.84650986|2022-01-01 15:00:00|14.666666666666666|\n",
      "|         20013496|lyft|2022|    1|  1|  16|           3|      5|         1|   13|40.86429404|-73.84650986|2022-01-01 16:00:00|12.666666666666666|\n",
      "|         20013497|lyft|2022|    1|  1|  17|           3|      5|         1|   11|40.86429404|-73.84650986|2022-01-01 17:00:00|              15.0|\n",
      "|         20013498|lyft|2022|    1|  1|  18|           3|      5|         1|    9|40.86429404|-73.84650986|2022-01-01 18:00:00|14.666666666666666|\n",
      "|         20013499|lyft|2022|    1|  1|  19|           3|      5|         1|   11|40.86429404|-73.84650986|2022-01-01 19:00:00|              10.0|\n",
      "+-----------------+----+----+-----+---+----+------------+-------+----------+-----+-----------+------------+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df = df.withColumn(\"is_holiday\", col(\"is_holiday\").cast(\"int\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3d41cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----+----+-----+---+----+------------+-------+----------+-----+-----------+------------+-------------------+------------------+\n",
      "|__index_level_0__|Name|year|month|day|hour|PULocationID|weekday|is_holiday|count|        lat|         lon|           TempTime|            countN|\n",
      "+-----------------+----+----+-----+---+----+------------+-------+----------+-----+-----------+------------+-------------------+------------------+\n",
      "|         20013480|   2|2022|    1|  1|   0|           3|      5|         1|   16|40.86429404|-73.84650986|2022-01-01 00:00:00|16.666666666666668|\n",
      "|         20013481|   2|2022|    1|  1|   1|           3|      5|         1|   21|40.86429404|-73.84650986|2022-01-01 01:00:00|              14.0|\n",
      "|         20013482|   2|2022|    1|  1|   2|           3|      5|         1|    6|40.86429404|-73.84650986|2022-01-01 02:00:00| 6.666666666666667|\n",
      "|         20013483|   2|2022|    1|  1|   3|           3|      5|         1|   11|40.86429404|-73.84650986|2022-01-01 03:00:00| 6.333333333333333|\n",
      "|         20013484|   2|2022|    1|  1|   4|           3|      5|         1|   10|40.86429404|-73.84650986|2022-01-01 04:00:00| 6.333333333333333|\n",
      "|         20013485|   2|2022|    1|  1|   5|           3|      5|         1|    7|40.86429404|-73.84650986|2022-01-01 05:00:00| 5.333333333333333|\n",
      "|         20013486|   2|2022|    1|  1|   6|           3|      5|         1|    9|40.86429404|-73.84650986|2022-01-01 06:00:00|               6.0|\n",
      "|         20013487|   2|2022|    1|  1|   7|           3|      5|         1|    6|40.86429404|-73.84650986|2022-01-01 07:00:00| 4.666666666666667|\n",
      "|         20013488|   2|2022|    1|  1|   8|           3|      5|         1|    5|40.86429404|-73.84650986|2022-01-01 08:00:00| 7.666666666666667|\n",
      "|         20013489|   2|2022|    1|  1|   9|           3|      5|         1|    7|40.86429404|-73.84650986|2022-01-01 09:00:00|               9.0|\n",
      "|         20013490|   2|2022|    1|  1|  10|           3|      5|         1|    2|40.86429404|-73.84650986|2022-01-01 10:00:00| 6.333333333333333|\n",
      "|         20013491|   2|2022|    1|  1|  11|           3|      5|         1|    5|40.86429404|-73.84650986|2022-01-01 11:00:00| 5.666666666666667|\n",
      "|         20013492|   2|2022|    1|  1|  12|           3|      5|         1|   11|40.86429404|-73.84650986|2022-01-01 12:00:00|10.333333333333334|\n",
      "|         20013493|   2|2022|    1|  1|  13|           3|      5|         1|    8|40.86429404|-73.84650986|2022-01-01 13:00:00|10.333333333333334|\n",
      "|         20013494|   2|2022|    1|  1|  14|           3|      5|         1|   10|40.86429404|-73.84650986|2022-01-01 14:00:00|11.666666666666666|\n",
      "|         20013495|   2|2022|    1|  1|  15|           3|      5|         1|   11|40.86429404|-73.84650986|2022-01-01 15:00:00|14.666666666666666|\n",
      "|         20013496|   2|2022|    1|  1|  16|           3|      5|         1|   13|40.86429404|-73.84650986|2022-01-01 16:00:00|12.666666666666666|\n",
      "|         20013497|   2|2022|    1|  1|  17|           3|      5|         1|   11|40.86429404|-73.84650986|2022-01-01 17:00:00|              15.0|\n",
      "|         20013498|   2|2022|    1|  1|  18|           3|      5|         1|    9|40.86429404|-73.84650986|2022-01-01 18:00:00|14.666666666666666|\n",
      "|         20013499|   2|2022|    1|  1|  19|           3|      5|         1|   11|40.86429404|-73.84650986|2022-01-01 19:00:00|              10.0|\n",
      "+-----------------+----+----+-----+---+----+------------+-------+----------+-----+-----------+------------+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.types import IntegerType\n",
    "mapping = {\n",
    "    'lyft': 2,\n",
    "    'uber': 3,\n",
    "    'yellow': 1\n",
    "}\n",
    "\n",
    "# 使用when和otherwise函数进行替换\n",
    "for key, value in mapping.items():\n",
    "    df = df.withColumn(\"Name\", when(df[\"Name\"] == key, value).otherwise(df[\"Name\"]))\n",
    "    \n",
    "df = df.withColumn(\"Name\", df[\"Name\"].cast(IntegerType()))    \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1180fc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----+----+-----+---+----+------------+-------+----------+-----+-----------+------------+------------------+\n",
      "|__index_level_0__|Name|year|month|day|hour|PULocationID|weekday|is_holiday|count|        lat|         lon|            countN|\n",
      "+-----------------+----+----+-----+---+----+------------+-------+----------+-----+-----------+------------+------------------+\n",
      "|         20013480|   2|2022|    1|  1|   0|           3|      5|         1|   16|40.86429404|-73.84650986|16.666666666666668|\n",
      "|         20013481|   2|2022|    1|  1|   1|           3|      5|         1|   21|40.86429404|-73.84650986|              14.0|\n",
      "|         20013482|   2|2022|    1|  1|   2|           3|      5|         1|    6|40.86429404|-73.84650986| 6.666666666666667|\n",
      "|         20013483|   2|2022|    1|  1|   3|           3|      5|         1|   11|40.86429404|-73.84650986| 6.333333333333333|\n",
      "|         20013484|   2|2022|    1|  1|   4|           3|      5|         1|   10|40.86429404|-73.84650986| 6.333333333333333|\n",
      "|         20013485|   2|2022|    1|  1|   5|           3|      5|         1|    7|40.86429404|-73.84650986| 5.333333333333333|\n",
      "|         20013486|   2|2022|    1|  1|   6|           3|      5|         1|    9|40.86429404|-73.84650986|               6.0|\n",
      "|         20013487|   2|2022|    1|  1|   7|           3|      5|         1|    6|40.86429404|-73.84650986| 4.666666666666667|\n",
      "|         20013488|   2|2022|    1|  1|   8|           3|      5|         1|    5|40.86429404|-73.84650986| 7.666666666666667|\n",
      "|         20013489|   2|2022|    1|  1|   9|           3|      5|         1|    7|40.86429404|-73.84650986|               9.0|\n",
      "|         20013490|   2|2022|    1|  1|  10|           3|      5|         1|    2|40.86429404|-73.84650986| 6.333333333333333|\n",
      "|         20013491|   2|2022|    1|  1|  11|           3|      5|         1|    5|40.86429404|-73.84650986| 5.666666666666667|\n",
      "|         20013492|   2|2022|    1|  1|  12|           3|      5|         1|   11|40.86429404|-73.84650986|10.333333333333334|\n",
      "|         20013493|   2|2022|    1|  1|  13|           3|      5|         1|    8|40.86429404|-73.84650986|10.333333333333334|\n",
      "|         20013494|   2|2022|    1|  1|  14|           3|      5|         1|   10|40.86429404|-73.84650986|11.666666666666666|\n",
      "|         20013495|   2|2022|    1|  1|  15|           3|      5|         1|   11|40.86429404|-73.84650986|14.666666666666666|\n",
      "|         20013496|   2|2022|    1|  1|  16|           3|      5|         1|   13|40.86429404|-73.84650986|12.666666666666666|\n",
      "|         20013497|   2|2022|    1|  1|  17|           3|      5|         1|   11|40.86429404|-73.84650986|              15.0|\n",
      "|         20013498|   2|2022|    1|  1|  18|           3|      5|         1|    9|40.86429404|-73.84650986|14.666666666666666|\n",
      "|         20013499|   2|2022|    1|  1|  19|           3|      5|         1|   11|40.86429404|-73.84650986|              10.0|\n",
      "+-----------------+----+----+-----+---+----+------------+-------+----------+-----+-----------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 删除不需要的列\n",
    "df = df.drop(\"TempTime\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "764638b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "行数: 6859080\n",
      "列数: 13\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# 假设您已经加载了 DataFrame df\n",
    "\n",
    "# 获取 DataFrame 的行数\n",
    "row_count = df.count()\n",
    "\n",
    "# 获取 DataFrame 的列数\n",
    "column_count = len(df.columns)\n",
    "\n",
    "# 打印行数和列数\n",
    "print(\"行数:\", row_count)\n",
    "print(\"列数:\", column_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45508e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----+----+-----+---+----+------------+-------+----------+-----+-----------+------------+-------------------+------------------+\n",
      "|__index_level_0__|Name|year|month|day|hour|PULocationID|weekday|is_holiday|count|        lat|         lon|           TempTime|            countN|\n",
      "+-----------------+----+----+-----+---+----+------------+-------+----------+-----+-----------+------------+-------------------+------------------+\n",
      "|         26872560|lyft|2023|    1|  1|   0|           3|      6|      true|   12|40.86429404|-73.84650986|2023-01-01 00:00:00|              15.0|\n",
      "|         26872561|lyft|2023|    1|  1|   1|           3|      6|      true|   28|40.86429404|-73.84650986|2023-01-01 01:00:00|15.333333333333334|\n",
      "|         26872562|lyft|2023|    1|  1|   2|           3|      6|      true|   19|40.86429404|-73.84650986|2023-01-01 02:00:00|              14.0|\n",
      "|         26872563|lyft|2023|    1|  1|   3|           3|      6|      true|   26|40.86429404|-73.84650986|2023-01-01 03:00:00|12.333333333333334|\n",
      "|         26872564|lyft|2023|    1|  1|   4|           3|      6|      true|   17|40.86429404|-73.84650986|2023-01-01 04:00:00| 9.333333333333334|\n",
      "|         26872565|lyft|2023|    1|  1|   5|           3|      6|      true|   14|40.86429404|-73.84650986|2023-01-01 05:00:00|               9.0|\n",
      "|         26872566|lyft|2023|    1|  1|   6|           3|      6|      true|   11|40.86429404|-73.84650986|2023-01-01 06:00:00| 7.333333333333333|\n",
      "|         26872567|lyft|2023|    1|  1|   7|           3|      6|      true|   10|40.86429404|-73.84650986|2023-01-01 07:00:00| 9.666666666666666|\n",
      "|         26872568|lyft|2023|    1|  1|   8|           3|      6|      true|    6|40.86429404|-73.84650986|2023-01-01 08:00:00| 6.333333333333333|\n",
      "|         26872569|lyft|2023|    1|  1|   9|           3|      6|      true|    6|40.86429404|-73.84650986|2023-01-01 09:00:00|               7.0|\n",
      "|         26872570|lyft|2023|    1|  1|  10|           3|      6|      true|    7|40.86429404|-73.84650986|2023-01-01 10:00:00|               9.0|\n",
      "|         26872571|lyft|2023|    1|  1|  11|           3|      6|      true|    8|40.86429404|-73.84650986|2023-01-01 11:00:00|11.333333333333334|\n",
      "|         26872572|lyft|2023|    1|  1|  12|           3|      6|      true|   14|40.86429404|-73.84650986|2023-01-01 12:00:00|12.666666666666666|\n",
      "|         26872573|lyft|2023|    1|  1|  13|           3|      6|      true|   11|40.86429404|-73.84650986|2023-01-01 13:00:00|11.333333333333334|\n",
      "|         26872574|lyft|2023|    1|  1|  14|           3|      6|      true|   11|40.86429404|-73.84650986|2023-01-01 14:00:00|              12.0|\n",
      "|         26872575|lyft|2023|    1|  1|  15|           3|      6|      true|   14|40.86429404|-73.84650986|2023-01-01 15:00:00|              20.0|\n",
      "|         26872576|lyft|2023|    1|  1|  16|           3|      6|      true|   15|40.86429404|-73.84650986|2023-01-01 16:00:00|14.666666666666666|\n",
      "|         26872577|lyft|2023|    1|  1|  17|           3|      6|      true|   11|40.86429404|-73.84650986|2023-01-01 17:00:00|14.333333333333334|\n",
      "|         26872578|lyft|2023|    1|  1|  18|           3|      6|      true|   11|40.86429404|-73.84650986|2023-01-01 18:00:00|16.333333333333332|\n",
      "|         26872579|lyft|2023|    1|  1|  19|           3|      6|      true|   19|40.86429404|-73.84650986|2023-01-01 19:00:00|              20.0|\n",
      "+-----------------+----+----+-----+---+----+------------+-------+----------+-----+-----------+------------+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------------+----+----+-----+---+----+------------+-------+----------+-----+-----------+------------+-------------------+------------------+\n",
      "|__index_level_0__|Name|year|month|day|hour|PULocationID|weekday|is_holiday|count|        lat|         lon|           TempTime|            countN|\n",
      "+-----------------+----+----+-----+---+----+------------+-------+----------+-----+-----------+------------+-------------------+------------------+\n",
      "|         26872560|lyft|2023|    1|  1|   0|           3|      6|         1|   12|40.86429404|-73.84650986|2023-01-01 00:00:00|              15.0|\n",
      "|         26872561|lyft|2023|    1|  1|   1|           3|      6|         1|   28|40.86429404|-73.84650986|2023-01-01 01:00:00|15.333333333333334|\n",
      "|         26872562|lyft|2023|    1|  1|   2|           3|      6|         1|   19|40.86429404|-73.84650986|2023-01-01 02:00:00|              14.0|\n",
      "|         26872563|lyft|2023|    1|  1|   3|           3|      6|         1|   26|40.86429404|-73.84650986|2023-01-01 03:00:00|12.333333333333334|\n",
      "|         26872564|lyft|2023|    1|  1|   4|           3|      6|         1|   17|40.86429404|-73.84650986|2023-01-01 04:00:00| 9.333333333333334|\n",
      "|         26872565|lyft|2023|    1|  1|   5|           3|      6|         1|   14|40.86429404|-73.84650986|2023-01-01 05:00:00|               9.0|\n",
      "|         26872566|lyft|2023|    1|  1|   6|           3|      6|         1|   11|40.86429404|-73.84650986|2023-01-01 06:00:00| 7.333333333333333|\n",
      "|         26872567|lyft|2023|    1|  1|   7|           3|      6|         1|   10|40.86429404|-73.84650986|2023-01-01 07:00:00| 9.666666666666666|\n",
      "|         26872568|lyft|2023|    1|  1|   8|           3|      6|         1|    6|40.86429404|-73.84650986|2023-01-01 08:00:00| 6.333333333333333|\n",
      "|         26872569|lyft|2023|    1|  1|   9|           3|      6|         1|    6|40.86429404|-73.84650986|2023-01-01 09:00:00|               7.0|\n",
      "|         26872570|lyft|2023|    1|  1|  10|           3|      6|         1|    7|40.86429404|-73.84650986|2023-01-01 10:00:00|               9.0|\n",
      "|         26872571|lyft|2023|    1|  1|  11|           3|      6|         1|    8|40.86429404|-73.84650986|2023-01-01 11:00:00|11.333333333333334|\n",
      "|         26872572|lyft|2023|    1|  1|  12|           3|      6|         1|   14|40.86429404|-73.84650986|2023-01-01 12:00:00|12.666666666666666|\n",
      "|         26872573|lyft|2023|    1|  1|  13|           3|      6|         1|   11|40.86429404|-73.84650986|2023-01-01 13:00:00|11.333333333333334|\n",
      "|         26872574|lyft|2023|    1|  1|  14|           3|      6|         1|   11|40.86429404|-73.84650986|2023-01-01 14:00:00|              12.0|\n",
      "|         26872575|lyft|2023|    1|  1|  15|           3|      6|         1|   14|40.86429404|-73.84650986|2023-01-01 15:00:00|              20.0|\n",
      "|         26872576|lyft|2023|    1|  1|  16|           3|      6|         1|   15|40.86429404|-73.84650986|2023-01-01 16:00:00|14.666666666666666|\n",
      "|         26872577|lyft|2023|    1|  1|  17|           3|      6|         1|   11|40.86429404|-73.84650986|2023-01-01 17:00:00|14.333333333333334|\n",
      "|         26872578|lyft|2023|    1|  1|  18|           3|      6|         1|   11|40.86429404|-73.84650986|2023-01-01 18:00:00|16.333333333333332|\n",
      "|         26872579|lyft|2023|    1|  1|  19|           3|      6|         1|   19|40.86429404|-73.84650986|2023-01-01 19:00:00|              20.0|\n",
      "+-----------------+----+----+-----+---+----+------------+-------+----------+-----+-----------+------------+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------------+----+----+-----+---+----+------------+-------+----------+-----+-----------+------------+-------------------+------------------+\n",
      "|__index_level_0__|Name|year|month|day|hour|PULocationID|weekday|is_holiday|count|        lat|         lon|           TempTime|            countN|\n",
      "+-----------------+----+----+-----+---+----+------------+-------+----------+-----+-----------+------------+-------------------+------------------+\n",
      "|         26872560|   2|2023|    1|  1|   0|           3|      6|         1|   12|40.86429404|-73.84650986|2023-01-01 00:00:00|              15.0|\n",
      "|         26872561|   2|2023|    1|  1|   1|           3|      6|         1|   28|40.86429404|-73.84650986|2023-01-01 01:00:00|15.333333333333334|\n",
      "|         26872562|   2|2023|    1|  1|   2|           3|      6|         1|   19|40.86429404|-73.84650986|2023-01-01 02:00:00|              14.0|\n",
      "|         26872563|   2|2023|    1|  1|   3|           3|      6|         1|   26|40.86429404|-73.84650986|2023-01-01 03:00:00|12.333333333333334|\n",
      "|         26872564|   2|2023|    1|  1|   4|           3|      6|         1|   17|40.86429404|-73.84650986|2023-01-01 04:00:00| 9.333333333333334|\n",
      "|         26872565|   2|2023|    1|  1|   5|           3|      6|         1|   14|40.86429404|-73.84650986|2023-01-01 05:00:00|               9.0|\n",
      "|         26872566|   2|2023|    1|  1|   6|           3|      6|         1|   11|40.86429404|-73.84650986|2023-01-01 06:00:00| 7.333333333333333|\n",
      "|         26872567|   2|2023|    1|  1|   7|           3|      6|         1|   10|40.86429404|-73.84650986|2023-01-01 07:00:00| 9.666666666666666|\n",
      "|         26872568|   2|2023|    1|  1|   8|           3|      6|         1|    6|40.86429404|-73.84650986|2023-01-01 08:00:00| 6.333333333333333|\n",
      "|         26872569|   2|2023|    1|  1|   9|           3|      6|         1|    6|40.86429404|-73.84650986|2023-01-01 09:00:00|               7.0|\n",
      "|         26872570|   2|2023|    1|  1|  10|           3|      6|         1|    7|40.86429404|-73.84650986|2023-01-01 10:00:00|               9.0|\n",
      "|         26872571|   2|2023|    1|  1|  11|           3|      6|         1|    8|40.86429404|-73.84650986|2023-01-01 11:00:00|11.333333333333334|\n",
      "|         26872572|   2|2023|    1|  1|  12|           3|      6|         1|   14|40.86429404|-73.84650986|2023-01-01 12:00:00|12.666666666666666|\n",
      "|         26872573|   2|2023|    1|  1|  13|           3|      6|         1|   11|40.86429404|-73.84650986|2023-01-01 13:00:00|11.333333333333334|\n",
      "|         26872574|   2|2023|    1|  1|  14|           3|      6|         1|   11|40.86429404|-73.84650986|2023-01-01 14:00:00|              12.0|\n",
      "|         26872575|   2|2023|    1|  1|  15|           3|      6|         1|   14|40.86429404|-73.84650986|2023-01-01 15:00:00|              20.0|\n",
      "|         26872576|   2|2023|    1|  1|  16|           3|      6|         1|   15|40.86429404|-73.84650986|2023-01-01 16:00:00|14.666666666666666|\n",
      "|         26872577|   2|2023|    1|  1|  17|           3|      6|         1|   11|40.86429404|-73.84650986|2023-01-01 17:00:00|14.333333333333334|\n",
      "|         26872578|   2|2023|    1|  1|  18|           3|      6|         1|   11|40.86429404|-73.84650986|2023-01-01 18:00:00|16.333333333333332|\n",
      "|         26872579|   2|2023|    1|  1|  19|           3|      6|         1|   19|40.86429404|-73.84650986|2023-01-01 19:00:00|              20.0|\n",
      "+-----------------+----+----+-----+---+----+------------+-------+----------+-----+-----------+------------+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------------+----+----+-----+---+----+------------+-------+----------+-----+-----------+------------+------------------+\n",
      "|__index_level_0__|Name|year|month|day|hour|PULocationID|weekday|is_holiday|count|        lat|         lon|            countN|\n",
      "+-----------------+----+----+-----+---+----+------------+-------+----------+-----+-----------+------------+------------------+\n",
      "|         26872560|   2|2023|    1|  1|   0|           3|      6|         1|   12|40.86429404|-73.84650986|              15.0|\n",
      "|         26872561|   2|2023|    1|  1|   1|           3|      6|         1|   28|40.86429404|-73.84650986|15.333333333333334|\n",
      "|         26872562|   2|2023|    1|  1|   2|           3|      6|         1|   19|40.86429404|-73.84650986|              14.0|\n",
      "|         26872563|   2|2023|    1|  1|   3|           3|      6|         1|   26|40.86429404|-73.84650986|12.333333333333334|\n",
      "|         26872564|   2|2023|    1|  1|   4|           3|      6|         1|   17|40.86429404|-73.84650986| 9.333333333333334|\n",
      "|         26872565|   2|2023|    1|  1|   5|           3|      6|         1|   14|40.86429404|-73.84650986|               9.0|\n",
      "|         26872566|   2|2023|    1|  1|   6|           3|      6|         1|   11|40.86429404|-73.84650986| 7.333333333333333|\n",
      "|         26872567|   2|2023|    1|  1|   7|           3|      6|         1|   10|40.86429404|-73.84650986| 9.666666666666666|\n",
      "|         26872568|   2|2023|    1|  1|   8|           3|      6|         1|    6|40.86429404|-73.84650986| 6.333333333333333|\n",
      "|         26872569|   2|2023|    1|  1|   9|           3|      6|         1|    6|40.86429404|-73.84650986|               7.0|\n",
      "|         26872570|   2|2023|    1|  1|  10|           3|      6|         1|    7|40.86429404|-73.84650986|               9.0|\n",
      "|         26872571|   2|2023|    1|  1|  11|           3|      6|         1|    8|40.86429404|-73.84650986|11.333333333333334|\n",
      "|         26872572|   2|2023|    1|  1|  12|           3|      6|         1|   14|40.86429404|-73.84650986|12.666666666666666|\n",
      "|         26872573|   2|2023|    1|  1|  13|           3|      6|         1|   11|40.86429404|-73.84650986|11.333333333333334|\n",
      "|         26872574|   2|2023|    1|  1|  14|           3|      6|         1|   11|40.86429404|-73.84650986|              12.0|\n",
      "|         26872575|   2|2023|    1|  1|  15|           3|      6|         1|   14|40.86429404|-73.84650986|              20.0|\n",
      "|         26872576|   2|2023|    1|  1|  16|           3|      6|         1|   15|40.86429404|-73.84650986|14.666666666666666|\n",
      "|         26872577|   2|2023|    1|  1|  17|           3|      6|         1|   11|40.86429404|-73.84650986|14.333333333333334|\n",
      "|         26872578|   2|2023|    1|  1|  18|           3|      6|         1|   11|40.86429404|-73.84650986|16.333333333333332|\n",
      "|         26872579|   2|2023|    1|  1|  19|           3|      6|         1|   19|40.86429404|-73.84650986|              20.0|\n",
      "+-----------------+----+----+-----+---+----+------------+-------+----------+-----+-----------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "行数: 3401352\n",
      "列数: 13\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.parquet('../time_series/TS6/TS6-2023.parquet')\n",
    "df1 = df1.select(\"__index_level_0__\", *df1.columns[:-1])\n",
    "df1.show()\n",
    "\n",
    "df1 = df1.withColumn(\"is_holiday\", col(\"is_holiday\").cast(\"int\"))\n",
    "df1.show()\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "mapping = {\n",
    "    'lyft': 2,\n",
    "    'uber': 3,\n",
    "    'yellow': 1\n",
    "}\n",
    "\n",
    "# 使用when和otherwise函数进行替换\n",
    "for key, value in mapping.items():\n",
    "    df1 = df1.withColumn(\"Name\", when(df1[\"Name\"] == key, value).otherwise(df1[\"Name\"]))\n",
    "df1 = df1.withColumn(\"Name\", df1[\"Name\"].cast(IntegerType()))        \n",
    "df1.show()\n",
    "\n",
    "df1 = df1.drop(\"TempTime\")\n",
    "df1.show()\n",
    "\n",
    "# 假设您已经加载了 DataFrame df\n",
    "\n",
    "# 获取 DataFrame 的行数\n",
    "row_count = df1.count()\n",
    "\n",
    "# 获取 DataFrame 的列数\n",
    "column_count = len(df1.columns)\n",
    "\n",
    "# 打印行数和列数\n",
    "print(\"行数:\", row_count)\n",
    "print(\"列数:\", column_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ca7b9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----+----+-----+---+----+------------+-------+----------+-----+-----------+------------+------------------+\n",
      "|__index_level_0__|Name|year|month|day|hour|PULocationID|weekday|is_holiday|count|        lat|         lon|            countN|\n",
      "+-----------------+----+----+-----+---+----+------------+-------+----------+-----+-----------+------------+------------------+\n",
      "|         20013480|   2|2022|    1|  1|   0|           3|      5|         1|   16|40.86429404|-73.84650986|16.666666666666668|\n",
      "|         20013481|   2|2022|    1|  1|   1|           3|      5|         1|   21|40.86429404|-73.84650986|              14.0|\n",
      "|         20013482|   2|2022|    1|  1|   2|           3|      5|         1|    6|40.86429404|-73.84650986| 6.666666666666667|\n",
      "|         20013483|   2|2022|    1|  1|   3|           3|      5|         1|   11|40.86429404|-73.84650986| 6.333333333333333|\n",
      "|         20013484|   2|2022|    1|  1|   4|           3|      5|         1|   10|40.86429404|-73.84650986| 6.333333333333333|\n",
      "|         20013485|   2|2022|    1|  1|   5|           3|      5|         1|    7|40.86429404|-73.84650986| 5.333333333333333|\n",
      "|         20013486|   2|2022|    1|  1|   6|           3|      5|         1|    9|40.86429404|-73.84650986|               6.0|\n",
      "|         20013487|   2|2022|    1|  1|   7|           3|      5|         1|    6|40.86429404|-73.84650986| 4.666666666666667|\n",
      "|         20013488|   2|2022|    1|  1|   8|           3|      5|         1|    5|40.86429404|-73.84650986| 7.666666666666667|\n",
      "|         20013489|   2|2022|    1|  1|   9|           3|      5|         1|    7|40.86429404|-73.84650986|               9.0|\n",
      "|         20013490|   2|2022|    1|  1|  10|           3|      5|         1|    2|40.86429404|-73.84650986| 6.333333333333333|\n",
      "|         20013491|   2|2022|    1|  1|  11|           3|      5|         1|    5|40.86429404|-73.84650986| 5.666666666666667|\n",
      "|         20013492|   2|2022|    1|  1|  12|           3|      5|         1|   11|40.86429404|-73.84650986|10.333333333333334|\n",
      "|         20013493|   2|2022|    1|  1|  13|           3|      5|         1|    8|40.86429404|-73.84650986|10.333333333333334|\n",
      "|         20013494|   2|2022|    1|  1|  14|           3|      5|         1|   10|40.86429404|-73.84650986|11.666666666666666|\n",
      "|         20013495|   2|2022|    1|  1|  15|           3|      5|         1|   11|40.86429404|-73.84650986|14.666666666666666|\n",
      "|         20013496|   2|2022|    1|  1|  16|           3|      5|         1|   13|40.86429404|-73.84650986|12.666666666666666|\n",
      "|         20013497|   2|2022|    1|  1|  17|           3|      5|         1|   11|40.86429404|-73.84650986|              15.0|\n",
      "|         20013498|   2|2022|    1|  1|  18|           3|      5|         1|    9|40.86429404|-73.84650986|14.666666666666666|\n",
      "|         20013499|   2|2022|    1|  1|  19|           3|      5|         1|   11|40.86429404|-73.84650986|              10.0|\n",
      "+-----------------+----+----+-----+---+----+------------+-------+----------+-----+-----------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------------+----+----+-----+---+----+------------+-------+----------+-----+-----------+------------+------------------+\n",
      "|__index_level_0__|Name|year|month|day|hour|PULocationID|weekday|is_holiday|count|        lat|         lon|            countN|\n",
      "+-----------------+----+----+-----+---+----+------------+-------+----------+-----+-----------+------------+------------------+\n",
      "|         21917736|   2|2022|   11|  1|   0|           3|      1|         0|   11|40.86429404|-73.84650986| 7.666666666666667|\n",
      "|         21917737|   2|2022|   11|  1|   1|           3|      1|         0|    6|40.86429404|-73.84650986| 4.333333333333333|\n",
      "|         21917738|   2|2022|   11|  1|   2|           3|      1|         0|    4|40.86429404|-73.84650986| 4.333333333333333|\n",
      "|         21917739|   2|2022|   11|  1|   3|           3|      1|         0|    2|40.86429404|-73.84650986|               2.0|\n",
      "|         21917740|   2|2022|   11|  1|   4|           3|      1|         0|    4|40.86429404|-73.84650986| 4.333333333333333|\n",
      "|         21917741|   2|2022|   11|  1|   5|           3|      1|         0|    9|40.86429404|-73.84650986| 9.666666666666666|\n",
      "|         21917742|   2|2022|   11|  1|   6|           3|      1|         0|   15|40.86429404|-73.84650986|14.666666666666666|\n",
      "|         21917743|   2|2022|   11|  1|   7|           3|      1|         0|   30|40.86429404|-73.84650986|25.666666666666668|\n",
      "|         21917744|   2|2022|   11|  1|   8|           3|      1|         0|   20|40.86429404|-73.84650986|19.333333333333332|\n",
      "|         21917745|   2|2022|   11|  1|   9|           3|      1|         0|   21|40.86429404|-73.84650986|              21.0|\n",
      "|         21917746|   2|2022|   11|  1|  10|           3|      1|         0|   17|40.86429404|-73.84650986|15.333333333333334|\n",
      "|         21917747|   2|2022|   11|  1|  11|           3|      1|         0|   16|40.86429404|-73.84650986|14.666666666666666|\n",
      "|         21917748|   2|2022|   11|  1|  12|           3|      1|         0|   12|40.86429404|-73.84650986|              17.0|\n",
      "|         21917749|   2|2022|   11|  1|  13|           3|      1|         0|    9|40.86429404|-73.84650986|              11.0|\n",
      "|         21917750|   2|2022|   11|  1|  14|           3|      1|         0|   17|40.86429404|-73.84650986|14.333333333333334|\n",
      "|         21917751|   2|2022|   11|  1|  15|           3|      1|         0|   18|40.86429404|-73.84650986|              16.0|\n",
      "|         21917752|   2|2022|   11|  1|  16|           3|      1|         0|   21|40.86429404|-73.84650986|              19.0|\n",
      "|         21917753|   2|2022|   11|  1|  17|           3|      1|         0|   13|40.86429404|-73.84650986|17.666666666666668|\n",
      "|         21917754|   2|2022|   11|  1|  18|           3|      1|         0|   16|40.86429404|-73.84650986|18.333333333333332|\n",
      "|         21917755|   2|2022|   11|  1|  19|           3|      1|         0|   20|40.86429404|-73.84650986|16.666666666666668|\n",
      "+-----------------+----+----+-----+---+----+------------+-------+----------+-----+-----------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------------+----+----+-----+---+----+------------+-------+----------+-----+-----------+------------+------------------+\n",
      "|__index_level_0__|Name|year|month|day|hour|PULocationID|weekday|is_holiday|count|        lat|         lon|            countN|\n",
      "+-----------------+----+----+-----+---+----+------------+-------+----------+-----+-----------+------------+------------------+\n",
      "|         26872560|   2|2023|    1|  1|   0|           3|      6|         1|   12|40.86429404|-73.84650986|              15.0|\n",
      "|         26872561|   2|2023|    1|  1|   1|           3|      6|         1|   28|40.86429404|-73.84650986|15.333333333333334|\n",
      "|         26872562|   2|2023|    1|  1|   2|           3|      6|         1|   19|40.86429404|-73.84650986|              14.0|\n",
      "|         26872563|   2|2023|    1|  1|   3|           3|      6|         1|   26|40.86429404|-73.84650986|12.333333333333334|\n",
      "|         26872564|   2|2023|    1|  1|   4|           3|      6|         1|   17|40.86429404|-73.84650986| 9.333333333333334|\n",
      "|         26872565|   2|2023|    1|  1|   5|           3|      6|         1|   14|40.86429404|-73.84650986|               9.0|\n",
      "|         26872566|   2|2023|    1|  1|   6|           3|      6|         1|   11|40.86429404|-73.84650986| 7.333333333333333|\n",
      "|         26872567|   2|2023|    1|  1|   7|           3|      6|         1|   10|40.86429404|-73.84650986| 9.666666666666666|\n",
      "|         26872568|   2|2023|    1|  1|   8|           3|      6|         1|    6|40.86429404|-73.84650986| 6.333333333333333|\n",
      "|         26872569|   2|2023|    1|  1|   9|           3|      6|         1|    6|40.86429404|-73.84650986|               7.0|\n",
      "|         26872570|   2|2023|    1|  1|  10|           3|      6|         1|    7|40.86429404|-73.84650986|               9.0|\n",
      "|         26872571|   2|2023|    1|  1|  11|           3|      6|         1|    8|40.86429404|-73.84650986|11.333333333333334|\n",
      "|         26872572|   2|2023|    1|  1|  12|           3|      6|         1|   14|40.86429404|-73.84650986|12.666666666666666|\n",
      "|         26872573|   2|2023|    1|  1|  13|           3|      6|         1|   11|40.86429404|-73.84650986|11.333333333333334|\n",
      "|         26872574|   2|2023|    1|  1|  14|           3|      6|         1|   11|40.86429404|-73.84650986|              12.0|\n",
      "|         26872575|   2|2023|    1|  1|  15|           3|      6|         1|   14|40.86429404|-73.84650986|              20.0|\n",
      "|         26872576|   2|2023|    1|  1|  16|           3|      6|         1|   15|40.86429404|-73.84650986|14.666666666666666|\n",
      "|         26872577|   2|2023|    1|  1|  17|           3|      6|         1|   11|40.86429404|-73.84650986|14.333333333333334|\n",
      "|         26872578|   2|2023|    1|  1|  18|           3|      6|         1|   11|40.86429404|-73.84650986|16.333333333333332|\n",
      "|         26872579|   2|2023|    1|  1|  19|           3|      6|         1|   19|40.86429404|-73.84650986|              20.0|\n",
      "+-----------------+----+----+-----+---+----+------------+-------+----------+-----+-----------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 创建一个Spark会话\n",
    "spark = SparkSession.builder.appName(\"MySparkSession\").getOrCreate()\n",
    "\n",
    "# 假设您已经加载了 DataFrame df\n",
    "\n",
    "# 筛选训练集数据\n",
    "train_data = df.filter(df[\"month\"] < 11)\n",
    "train_data.show()\n",
    "\n",
    "# 筛选测试集数据\n",
    "test_data = df.filter(df[\"month\"] > 10)\n",
    "test_data.show()\n",
    "\n",
    "val_data = df1\n",
    "val_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70a18616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取特征列和标签列\n",
    "feature_columns = df.columns\n",
    "feature_columns.remove(\"count\")\n",
    "feature_columns.remove(\"countN\")\n",
    "\n",
    "X_train = train_data.select(feature_columns)\n",
    "y_train = train_data.select(\"countN\")\n",
    "X_test = test_data.select(feature_columns)\n",
    "y_test = test_data.select(\"count\")\n",
    "X_val = val_data.select(feature_columns)\n",
    "y_val = val_data.select(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60cbd46c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('__index_level_0__', 'bigint'),\n",
       " ('Name', 'int'),\n",
       " ('year', 'bigint'),\n",
       " ('month', 'bigint'),\n",
       " ('day', 'bigint'),\n",
       " ('hour', 'bigint'),\n",
       " ('PULocationID', 'bigint'),\n",
       " ('weekday', 'int'),\n",
       " ('is_holiday', 'int'),\n",
       " ('lat', 'double'),\n",
       " ('lon', 'double')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e4dd6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('__index_level_0__', 'bigint'),\n",
       " ('Name', 'int'),\n",
       " ('year', 'bigint'),\n",
       " ('month', 'bigint'),\n",
       " ('day', 'bigint'),\n",
       " ('hour', 'bigint'),\n",
       " ('PULocationID', 'bigint'),\n",
       " ('weekday', 'int'),\n",
       " ('is_holiday', 'int'),\n",
       " ('lat', 'double'),\n",
       " ('lon', 'double')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4730771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('__index_level_0__', 'bigint'),\n",
       " ('Name', 'int'),\n",
       " ('year', 'bigint'),\n",
       " ('month', 'bigint'),\n",
       " ('day', 'bigint'),\n",
       " ('hour', 'bigint'),\n",
       " ('PULocationID', 'bigint'),\n",
       " ('weekday', 'int'),\n",
       " ('is_holiday', 'int'),\n",
       " ('lat', 'double'),\n",
       " ('lon', 'double')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29e7a26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start時間: 2023-09-05 10:32:43.270686\n",
      "------------------\n",
      "+------------------+\n",
      "|             label|\n",
      "+------------------+\n",
      "|16.666666666666668|\n",
      "|              14.0|\n",
      "| 6.666666666666667|\n",
      "| 6.333333333333333|\n",
      "| 6.333333333333333|\n",
      "| 5.333333333333333|\n",
      "|               6.0|\n",
      "| 4.666666666666667|\n",
      "| 7.666666666666667|\n",
      "|               9.0|\n",
      "| 6.333333333333333|\n",
      "| 5.666666666666667|\n",
      "|10.333333333333334|\n",
      "|10.333333333333334|\n",
      "|11.666666666666666|\n",
      "|14.666666666666666|\n",
      "|12.666666666666666|\n",
      "|              15.0|\n",
      "|14.666666666666666|\n",
      "|              10.0|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# 输出开始时间\n",
    "print(\"start時間:\", datetime.datetime.now())\n",
    "print('------------------')\n",
    "\n",
    "# 创建VectorAssembler来合并特征列为一个Vector列\n",
    "assembler = VectorAssembler(inputCols=X_train.columns, outputCol=\"features\")\n",
    "assembler_test = VectorAssembler(inputCols=X_test.columns, outputCol=\"features_test\")\n",
    "X_train = assembler.transform(X_train)\n",
    "X_test = assembler_test.transform(X_test)\n",
    "X_val = assembler_test.transform(X_val)\n",
    "\n",
    "\n",
    "# 创建 MinMaxScaler\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "scaler_test = MinMaxScaler(inputCol=\"features_test\", outputCol=\"scaled_features_test\")\n",
    "\n",
    "\n",
    "# 估计缩放器的参数\n",
    "scaler_model = scaler.fit(X_train)\n",
    "scaler_model_test = scaler_test.fit(X_test)\n",
    "\n",
    "# 使用缩放器将数据进行转换\n",
    "X_train_scaled = scaler_model.transform(X_train)\n",
    "X_test_scaled = scaler_model_test.transform(X_test)\n",
    "X_val_scaled = scaler_model_test.transform(X_val)\n",
    "\n",
    "# 添加标签列到DataFrame\n",
    "y_train_scaled = y_train.withColumnRenamed(\"countN\", \"label\")\n",
    "y_test_scaled = y_test.withColumnRenamed(\"count\", \"label\")\n",
    "y_val_scaled = y_val.withColumnRenamed(\"count\", \"label\")\n",
    "\n",
    "\n",
    "y_train_scaled.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8aa75069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|     scaled_features|\n",
      "+--------------------+\n",
      "|[0.0,0.5,0.5,0.0,...|\n",
      "|[1.54393061575812...|\n",
      "|[3.08786123151625...|\n",
      "|[4.63179184727438...|\n",
      "|[6.17572246303251...|\n",
      "|[7.71965307879063...|\n",
      "|[9.26358369454876...|\n",
      "|[1.08075143103068...|\n",
      "|[1.23514449260650...|\n",
      "|[1.38953755418231...|\n",
      "|[1.54393061575812...|\n",
      "|[1.69832367733394...|\n",
      "|[1.85271673890975...|\n",
      "|[2.00710980048556...|\n",
      "|[2.16150286206137...|\n",
      "|[2.31589592363719...|\n",
      "|[2.47028898521300...|\n",
      "|[2.62468204678881...|\n",
      "|[2.77907510836463...|\n",
      "|[2.93346816994044...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled.select('scaled_features').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4426b5c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 48.0 failed 1 times, most recent failure: Lost task 3.0 in stage 48.0 (TID 161) (P215-2203-NB01 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:571)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:534)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 15 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:571)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:534)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 15 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mX_train_scaled\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscaled_features\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatMap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 创建随机森林回归模型\u001b[39;00m\n\u001b[0;32m      5\u001b[0m rforest \u001b[38;5;241m=\u001b[39m RandomForestRegressor(numTrees\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, minInstancesPerNode\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fhvhv\\lib\\site-packages\\pyspark\\rdd.py:1814\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1812\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[0;32m   1813\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1814\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fhvhv\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fhvhv\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fhvhv\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 48.0 failed 1 times, most recent failure: Lost task 3.0 in stage 48.0 (TID 161) (P215-2203-NB01 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:571)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:534)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 15 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:571)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:534)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 15 more\r\n"
     ]
    }
   ],
   "source": [
    "train_data = X_train_scaled.select('scaled_features').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "\n",
    "# 创建随机森林回归模型\n",
    "rforest = RandomForestRegressor(numTrees=1000, seed=0, minInstancesPerNode=20)\n",
    "\n",
    "# 创建一个Pipeline来执行特征向量化和模型训练\n",
    "# pipeline = Pipeline(stages=[rforest])\n",
    "\n",
    "# 计时开始\n",
    "start_time = time.time()\n",
    "\n",
    "# 训练模型\n",
    "model = rforest.fit(train_data)\n",
    "\n",
    "# 停止计时\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(\"\\n程序执行花费的时间：\", round(execution_time, 2), \"秒\")\n",
    "print(\"done時間:\", datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226a9efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "# 使用VectorAssembler创建特征向量\n",
    "assembler = VectorAssembler(inputCols=X_train.columns, outputCol=\"features\")\n",
    "X_train = assembler.transform(X_train)\n",
    "X_test = assembler.transform(X_test)\n",
    "X_val = assembler.transform(X_val)\n",
    "\n",
    "# 创建 MinMaxScaler\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "\n",
    "# 估计缩放器的参数\n",
    "scaler_model = scaler.fit(X_train)\n",
    "\n",
    "# 使用缩放器将数据进行转换\n",
    "X_train_scaled = scaler_model.transform(X_train)\n",
    "X_test_scaled = scaler_model.transform(X_test)\n",
    "X_val_scaled = scaler_model.transform(X_val)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
